# Hand Gesture Classification Using MediaPipe Landmarks from the HaGRID Dataset

## Overview

This project focuses on classifying hand gestures using landmark data generated by **MediaPipe** from the **HaGRID (Hand Gesture Recognition Image Dataset)**. The input is a CSV file containing hand landmarks (`x`, `y`, `z` coordinates) and gesture labels. The final output is a trained machine learning model capable of accurately recognizing hand gestures.

![Capture](https://github.com/user-attachments/assets/3f3a14fa-77fa-49e0-bd62-f220d328a990)

The project mainly consists of:
- Data preprocessing and visualization
- Training multiple ML models (SVM, XGBoost, and Random Forest)
- Hyperparameter tuning for the models to get the best results available
- Evaluating and comparing model performance
- Presenting results through code and an output video
- **MLflow integration for experiment tracking and model management**

---

## Dataset Details

- **Source**: HaGRID Dataset
- **Classes**: 18 distinct hand gestures
- **Features**: 21 landmarks per hand (each with `x`, `y`, `z` coordinates)

---

## Project Deliverables

### 1. Jupyter Notebook
- Contains the code and the implementation for data exploration, preprocessing, and model evaluation.

### 2. Output Video
- **Demo Link**: [Google Drive Demo](https://drive.google.com/file/d/1e-k6xg07kFS20H6oAt7aFrIbl0aZXfLO/view?usp=sharing)
- A short video demonstrating real-time gesture recognition using the trained model and MediaPipe hand tracking.

### 3. MLflow Training Script
- `src/train.py` contains all steps as functions:
  - Data loading, preprocessing, normalization
  - Data splitting
  - Model training with hyperparameter tuning (GridSearchCV)
  - MLflow logging of parameters, metrics, models, and artifacts (confusion matrices, dataset)
- Run with:
  ```sh
  python src/train.py
  ```
- MLflow UI for experiment tracking:
  ```sh
  mlflow ui
  ```
  Then visit [http://localhost:5000](http://localhost:5000) in your browser.

---

## Requirements

- Python 3.10
- MediaPipe
- NumPy, pandas, matplotlib, seaborn
- scikit-learn
- xgboost
- mlflow
- Jupyter Notebook

### Virtual Environment Setup

It is recommended to use a virtual environment for this project:

```sh
python -m venv venv
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate

pip install -r requirements.txt
```

---

## Usage

1. **Prepare the dataset**  
   Place your `hand_landmarks_data.csv` in the `dataset/` directory.

2. **Train and log models with MLflow**  
   ```sh
   python src/train.py
   ```

3. **Track experiments**  
   Start the MLflow UI:
   ```sh
   mlflow ui
   ```
   Open [http://localhost:5000](http://localhost:5000) to view experiment runs, metrics, and artifacts.

4. **Real-time Inference**  
   Use the provided notebook or scripts to run real-time gesture recognition with your webcam.

---

## Model Comparison

After training, the following table summarizes the validation performance of each model:

| Model         | Accuracy | F1 Score | Precision |
|---------------|----------|----------|-----------|
| SVM           | 0.9898   | 0.9898   | 0.9898    |
| XGBoost       | 0.9848   | 0.9848   | 0.9848    |
| Random Forest | 0.9797   | 0.9797   | 0.9797    |


**Model Comparison Graph:**

![Model Comparison](assets/model_comparison.png)

---

## Model Selection and Explanation

The **SVM model** was selected as the best-performing model based on its highest accuracy, precision and F1-score on the validation set. SVM consistently outperformed XGBoost and Random Forest in the three metrics, making it the most reliable choice for real-time hand gesture recognition in this project.

---

## Results

- All models are evaluated and compared using accuracy, F1-score, and precision.
- Best models and confusion matrices are logged as MLflow artifacts.
- SVM model achieved the highest performance and is used for real-time inference.

---

## Acknowledgements

- [HaGRID Dataset](https://hagrid-data.github.io/)
- [MediaPipe](https://mediapipe.dev/)
- [MLflow](https://mlflow.org/)

---